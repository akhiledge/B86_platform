Writing an efficient program requires two types of activities. First, we must select the best set of algorithms
and data structures. Second, we must write source code that the compiler can effectively optimize to turn into
efficient executable code. For this second part, it is important to understand the capabilities and limitations of
optimizing compilers. Seemingly minor changes in how a program is written can make large differences in
how well a compiler can optimize it. Some programming languages are more easily optimized than others.
Some features of C, such as the ability to perform pointer arithmetic and casting, make it challenging to
optimize. Programmers can often write their programs in ways that make it easier for compilers to generate
efficient code.
In approaching the issue of program development and optimization, we must consider how the code will
be used and what critical factors affect it. In general, programmers must make a trade-off between how
easy a program is to implement and maintain, and how fast it will run. At an algorithmic level, a simple
insertion sort can be programmed in a matter of minutes, whereas a highly efficient sort routine may take a
day or more to implement and optimize. At the coding level, many low-level optimizations tend to reduce
code readability and modularity. This makes the programs more susceptible to bugs and more difficult to
modify or extend. For a program that will just be run once to generate a set of data points, it is more
important to write it in a way that minimizes programming effort and ensures correctness. For code that
will be executed repeatedly in a performance-critical environment, such as in a network router, much more
extensive optimization may be appropriate.
In this chapter, we describe a number of techniques for improving code performance. Ideally, a compiler
would be able to take whatever code we write and generate the most efficient possible machine-level pro-
gram having the specified behavior. In reality, compilers can only perform limited transformations of the
program, and they can be thwarted by optimization blockers—aspects of the program whose behavior de-
pends strongly on the execution environment. Programmers must assist the compiler by writing code that
can be optimized readily. In the compiler literature, optimization techniques are classified as either “ma-
chine independent,” meaning that they should be applied regardless of the characteristics of the computer
that will execute the code, or as “machine dependent,” meaning they depend on many low-level details of
the machine. We organize our presentation along similar lines, starting with program transformations that
should be standard practice when writing any program. We then progress to transformations whose efficacy
depends on the characteristics of the target machine and compiler.
These transformations also tend to reduce the modularity and readability of the code and hence should be
applied when maximum performance is thedominant concern.
To maximize the performance of a program, both the programmer and the compiler need to have a model of
the target machine, specifying how instructions are processed and the timing characteristics of the different
operations. For example, the compiler must know timing information to be able to decide whether it is
should use a multiply instruction or some combinations of shifts and adds. Modern computers use sophisti-
cated techniques to process a machine-level program, executing many instructions in parallel and possibly
in a different order than they appear in the program. Programmers must understand how these processors
work to be able to tune their programs for maximum speed. We present a high-level model of such a ma-
chine based on some recent models of Intel processors. We devise a graphical notation that can be used to
visualize the execution of instructions on the processor and to predict program performance.
We conclude by discussing issues related to optimizing large programs. We describe the use of code
profilers—tools that measure the performance of different parts of a program. This analysis can help find
inefficiencies in the code and identify the parts of the program on which we should focus our optimization
efforts. Finally, we present an important observation, known as Amdahl’s Law quantifying the overall effect
of optimizing some portion of a system.
In this presentation, we make code optimization look like a simple, linear process of applying a series
of transformations to the code in a particular order. In fact, the task is not nearly so straightforward. A
fair amount of trial-and-error experimentation is required. This is especially true as we approach the later
optimization stages, where seemingly small changes can cause major changes in performance, while some
very promising techniques prove ineffective. As we will see in the examples, it can be difficult to explain
exactly why a particular code sequence has a particular execution time. Performance can depend on many
detailed features of the processor design for which we have relatively little documentation or understanding.
This is another reason to try a number of different variations and combinations of techniques.
Studying the assembly code is one of the most effective means of gaining some understanding of the com-
piler and how the generated code will run. A good strategy is to start by looking carefully at the code for
the inner loops. One can identify performance-reducing attributes such as excessive memory references
and poor use of registers. Starting with the assembly code, we can even predict what operations will be
performed in parallel and how well they will use the processor resources.
